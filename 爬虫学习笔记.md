# 爬虫学习笔记

[TOC]





## 第一章 网络请求



### 一. HTTP协议和chrome抓包工具

略

### 二. urllib库简单用法

1. request库

    * `request.urlopen()`
        			返回html的response。 
        			返回对象可以有 read(size), readline, redlines以及 getcode 方法

    * `request.urlretrieve(网页的url, 下载名字)`
                    把网页上的一个文件保存到本地 

2. parse库

    * `parse.urlencode(key：value)`
        如果url有中文或者其他特殊字符，那么浏览器会自动给我们进行编码，但是通过爬虫来访问的时候，就必须用手动来进行编码。 这个方法可以将一个字典编码成key=value形式，且都用ascii码表示

        例如：这个方法可以将中文编码成%0x形式表示=

        tips：这种方法可以直接用 .encode() 方法代替。

    * `parse.parse_qs()`
        这个相当于urlencode的反函数，解码

    * `parse.urlparse()` 和 `parse.urlsplit()`

        顾名思义，这2个函数是对url进行解析
        
        不同点，前者urlparse 会多返回一个params的参数。
        这个params用的很少

3. request库中的Request类。  request.Request
    *如果想要在请求的时候增加一些请求头，那么则必须使用request.Request类来实现。*
    
4. ProxyHandler 处理器（代理设置）

    很多网站会检测多次访问的ip地址。如果访问太多可能会加入黑名单。
    如何解决？

```python
from urllib import request
url = 'http://httpbin.org/ip'
# 用ProxyHandler传入代理构建一个handler
handler = request.ProxyHandler({"http":"27.43.190.180:9999"})
# 使用上面创建的handler构建一个opener
opener = request.build_opener(handler)
# 用opener去发送一个请求
resp = opener.open(url)
print(resp.read())

# 注： 现在貌似没用了
```

5. **对Cookie的操作（基于urllib）**

    1. cookielib库和HTTPCookieProcessor 模拟登陆

        Cookie是指网站服务器为了辨别用户身份和进行session跟踪。而储存在用户浏览器上的文本文件，cookie可以保持登陆信息到用户等下次与服务器的session

        * 方法一：直接 把网页登陆后的Cookie复制到请求头上传进去。

        * **方法二：爬虫自动登陆授权页面**

            用到的模块：**http.cookiejar模块**和HTTPCookieProcessor

            用到的类：
            **CookieJar**： 管理HTTP cookie值，存储HTTP请求生成的cookie，向传出的HTTP请求添加cookie的对象。整个cookie都存储再内存中，对CookieJar实例进行垃圾回收后，cookie也将丢失
            **FileCookieJar(filename, delayload=None, policy=None)**：由CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储在文件当中。filename是存储cookie的文件名。delayload为true时，支持延时访问文件，即只有在需要的时候猜读取文件或者再文件中存储数据
            **MozilaCookieJar，LWPCookieJar**：都是由FileCookieJar派生而来。不同点：格式与兼容有区别。主要用MozilaCookieJar 。该类创建与Mozila浏览器cookie.txt兼容的FileCookieJar实例。

    2. 保存Cookie到本地
        保存cookie到本地，可以使用cookiejar的sava函数，并且需要指定一个文件名。
        然后对应还有一个load函数
        这两个函数的ignore_discard参数是指 加载或者保存过期的cookie

    ```python
    cookiejar = MozillaCookieJar("cookie.txt") # 火狐公司的cookie格式，文件名为cookie
    handler = request.HTTPCokkieProcessor(cookiejar)
    opener = request.build_opener(handler)
    headers = {...}
    req = request.Request(url, headers)
    resp = opener.open(req)
    print(resp.read())
    cookiejar.save(ignore_discard=True, ignore_expires=True)
    ```

### 三. requests库：

1. **发送GET请求**

    `response = requests.get("http...", params = kw, headers = headers)`  

    这里的params接受一个字典或者字符串的查询参数。**
    这样就能拿到这个url里的绝大部分东西了

    * response.text:
        服务器返回来的数据。text是unicode字符串
        *这个是将response.content进行解码的字符串，解码需要指定一个解码方式，requests会根据自己的猜测来判断解码的方式，所以有时候可能会猜测错误，就会导致乱码。这时候，就应该使用
        response.content.decode(‘utf-8’)或者其他方式进行手动解码*
    * response.content
        服务器返回来的字节流数据。**content是bytes数据类型！**
        未经过解码
    * response.url
        查看完整的url地址
    * response.encoding
        查看响应头部字符编码
    * response.status_code
        查看响应码

    Tips： 注意response.text和response.content。用错可能会产生乱码问题

2. **发POST请求**
    和get请求大同小异：
    只要通过requests.post(“url”, data=data, headers=headers, params=params)
    就行了

    tips：如果返回的是json数据，那么可以调用response.json().来将json字符串转换为字典或者列表

3. **使用代理：**
   
   只要在get或者post上面传递proxies参数就行
   
   proxies是一个字典的形式：
   举例：
   {
   	“http”:“111.111.111.111:9999”
   }
   
4. **Cookie** 和 **Session**

    * 获取cookie： response.cookies
        获取cookie（以字典方式）： response.cookie.get_dict()

    * 之前使用urllib库，是可以使用opener发送多个请求，多个请求之间是可以共享cookie的，那么如果使用requests，也要达到共享cookie的目的，那么可以使用requests库提供的session对象。这里的session只是一个会话的对象而已。
        session可以在多次请求中共享cookie

        

5. **处理不信任的SSL证书**
    对于那些已经被信任的SSL证书的网站，那么使用requests可以返回正常响应。
    如果是不信任的SSL证书的网站，
    则要加上一个参数：
    requests.get(url, **verify=False**)





## 第二章 数据提取

### 一. XPath 语法和lxml模块

xpath:(XML Path Language) 是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历
xpath 开发工具： 
	chrome插件 xpath Helper
	firefox插件 try xpath

* XPath 语法：

    1. 选取节点：
        XPath使用路径表达式来选取XML文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似

    |  表达式  | 描述                                                         | 示例           | 结果                            |
    | :------: | ------------------------------------------------------------ | -------------- | ------------------------------- |
    | nodename | 选取此节点的所有子节点                                       | bookstore      | 选取bookstore下所有的子节点     |
    |    /     | 如果是在最前面，代表从根节点选取。否则选择某节点下的某个节点 | /bookstore     | 选取根元素下所有的bookstore节点 |
    |    //    | 从全局节点中选取节点，随便在哪个位置                         | //book         | 从全局节点中找到所有的book节点  |
    |    @     | 选取某个节点的属性                                           | //book[@price] | 选取所有拥有price属性的book节点 |

    2. 谓语：

        谓语**用来查找某个特定节点或者包含某个指定的值的节点**，被嵌在方括号中。
        下面的表格中，列出了带有位于的一些路径表达式以及表达式的结果

        | 路径表达式                   | 描述                                         |
        | ---------------------------- | -------------------------------------------- |
        | /bookstore/book[1]           | 选取bookstore下的第一个子元素（下标从1开始） |
        | /bookstore/book[last()]      | 选取bookstore下的倒数第二个book元素          |
        | bookstore/book[position()<3] | 选取bookstore下前面两个子元素                |
        | //book[@price]               | 选取拥有price属性的book元素                  |
        | //book[@price=10]            | 选取所有属性price等于10的book元素            |

    3. 通配符
        *表示通配符

        | 通配符 | 描述                 | 示例         | 结果                        |
        | ------ | -------------------- | ------------ | --------------------------- |
        | *      | 匹配任意节点         | /bookstore/* | 选取bookstore下的所有子元素 |
        | @*     | 匹配节点中的任何属性 | //book[@*]   | 选取带有属性的book元素      |

    4. 逻辑

        |  &  +  -  mod等等


    总结： 
    
    * 注意/ 和// 的区别。是子节点和全部节点。 // 获取子孙节点。一般//用的多
    * contains：有时候某个属性中包含了多个值，那么可以通过contains函数：
        例如：`//div[contains(@class, "job_detail")]`
    * 谓词下标是从1开始的

* **lxml库**

    使用lxml解析HTML代码：

    1. 解析html字符串，使用`lxml.etree.HTML`进行解析。例如：

        ```python
        htmlElement = etree.HTML(text)
        print(etree.tostring(htmlElement, encoding="utf-8").decode("utf-8"))
        ```

    2. 解析html文件。使用`lxml.etree.parse`进行解析。
        注意，当文件中的html不规范的时候，注意更换解析器为HTMLParser。例如：

        ```python
        parser = etree.HTMLParser(encoding="utf-8")
        htmlElement = etree.parse("tencent.html", parser=parser)
        print(etree.tostring(htmlElement, encoding="utf-8").decode("utf-8"))
        ```

        

* lxml与XPath结合运用：
    详见代码10

### 二. BeautifulSoup4库

和lxml一样，BeautifuSoup也是一个HTML/XML的解析器。主要的功能也是如何解析和提取HTML/XML数据
lxml只会局部遍历。而BeautifulSoup是基于HTML DOM()的。会载入整个文档。解析整个DOM树，因此时间和内存开销大，性能低于lxml
但是非常简单。支持CSS选择器。和诸多解析器

```python
from  bs4 import BeautifuSoup
```

TIPS：    .prettify()   	以更美观的方式打印

**TIPS :**
**string, strings, stripper_strings属性以及get_text方法：**

1. string：获取某个标签下的非标签字符串。返回来的是个字符串
2. strings: 获取某个标签下的子孙非标签字符串。返回来的是一个生成器
3. stripper_strings: 获取某个标签下的子孙非标签字符串。会去掉空白字符。返回来的是一个生成器
4. get_text :获取某个标签下的子孙非标签字符串。不是以列表的形式返回，是以普通字符串返回



**搜索文档树**

1. **find和find_all 方法：**
    find方法：找到第一个满足条件的标签后立即返回，只返回一个元素（**tag类型**，**但打印的时候显示的是字符串**（会自动调用repr方法））
    find_all 方法：找到所有满足条件的标签。然后返回一个列表

    1. find_all在提取标签的时候，第一个参数是标签的名字，如果想用标签属性进行过滤的时候
    2. 有些时候，不想提取这么多。那么可以使用limit参数

    例子：

    ```python
    soup.find_all("a", attrs={"id":"link2"})
    soup.find_all("a", "id"="link2")
    ```

2. **select 方法**
    使用1方法可以方便的找出元素，但是有时候使用css选择器的方式可以更加方便。使用css选择器的语法，应该使用select方法。

    1. 通过标签名查找：
        `soup.select("a")`
    2. 通过类名查找
        通过类名，则应该在类的前面加一个`.` 
        `soup.select("".sister")`
    3. 通过id查找：
        通过id查找，则应该在id的名字前面加一个#号
        `soup.select("#link1")`
    4. 组合查找。
        1. 通过空格分开。会找到sister以内的所有子孙元素
            `soup.select(".sister #link1")`
            1. 通过 `>` 分开。则只会找到sister的子元素
    5. 根据属性的名字进行查找：
        应该险些标签名字。然后再在中括号钟写属性的值
        `soup.select(input[name="name"])`
        注意这里没有@, 与XPath区分一下。
    6. 在根据类名或者id进行查找的时候，如果还要根据标签名进行过滤。那么可以在类的前面或者id的前面加上标签名字：
        `soup.select("div.name")`
        `soup.select(div#name)`

TIPS: 详细的应用例子详见 代码

**Beautiful Soup 总结：**

1. **四种对象：**
    1. Tag ： BeautifulSoup中所有的标签都是Tag类型。并且BeautifulSoup的对象本质上也是一个Tag类型。所以其实一些方法比如find，find_all并不是BeautifulSoup的。而是Tag的
    2. NavigableString：继承自python中的str。用起来和python的str一样得
    3. BeautifulSoup：继承自Tag。用来生成BeautifulSoup的。
    4. Comment：继承自NavigabelString

2. **contents和children：**

    返回某个标签下的直接子元素。其中也包括字符串。
    它们两个的区别是：contents返回来的是一个列表。children返回的是一个迭代器

### 三. 正则表达式

略



## 第三章 数据文件处理



### 一. json文件处理

1. **JSON支持的数据格式：**

    * 对象(字典)： { }
    * 数组(列表)： [ ]
    * 整形，浮点型 
    * 字符串类型，字符串必须要双引号

    多个数据之间使用逗号分开
    **注意：JSON本质上就是一个字符串**

2. 字典,列表,JSON字符串，JSON文件相互转换
    主要用：dump,dumps,load,loads .
    详细用法略

### 二. CSV文件处理：

1. CSV文件介绍
    1. 纯文本，使用某个字符集等
    2. 由记录组成
    3. 每条记录被分隔符分割为字段（逗号，分号，空格，制表符）
    4. 每条记录都有同样的字段序列

2. CSV读写操作：详见代码CSV.py